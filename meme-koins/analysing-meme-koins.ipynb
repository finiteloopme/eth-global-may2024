# Analysing Meme Koins
1. Setup working environment
2. Create utilities
3. Analytics workflow
## 1. Setup working environment
# @title 1.1. Install package

# %pip install google-cloud-storage
# %pip install google-cloud-aiplatform
# %pip install bigframes

# Automatically restart kernel after installs so that your environment can access the new packages
# import IPython

# app = IPython.Application.instance()
# app.kernel.do_shutdown(True)
# @title 1.2. Authenticate the user
from google.colab import auth
auth.authenticate_user()
# @title 1.3. GCP Project Configuration

GCP_PROJECT="gcda-apac-sc"
GCP_REGION="us-central1"
SOL_DATA="bigquery-public-data.crypto_solana_mainnet_us"
## 2. Create utilities
# @title 2.1. AIPlatform Class to work with Generative AI

import vertexai
import pandas as pd
from vertexai.generative_models import (
    GenerationConfig,
    GenerativeModel,
)

DEFAULT_GENAI_MODEL_ID="gemini-1.5-pro-preview-0409"

class AIPlatform:
    """A client for interacting with Vertex AI."""
    project_id: str
    region: str

    def __init__(self, project_id, region, model_id=DEFAULT_GENAI_MODEL_ID):
        """Initializes the AIPlatform client.

        Args:
        project_id: The ID of the project that the client will use.
        region: The region that the client will use.
        model_id: The ID of the model that the client will use.
        """
        project_id = project_id
        region = region
        # Initialize Vertex AI endpoint for the given project and region
        vertexai.init(project=project_id, location=region)
        self.model = GenerativeModel(
            model_id,
            system_instruction=[
                "You are a web3 and crypto expert",
                "You are not expected to explain your answers",
                "Respond in pure JSON in format {symbol: category}",
                "Respond by picking one of the below 10 categories for each input web3 token",
                # "Your mission is to recommend only one of the below 10 categories for the input web3 token: ",
                    "Animal-Based",
                    "Popular Culture",
                    "Social Media",
                    "Financial and Technological",
                    "Food and Beverage",
                    "Sports and Esports",
                    "Political and Social Commentary",
                    "Utility and Functionality",
                    "Community-Driven Projects",
                    "Humorous and Absurdist Themes"
                ]
            )

    def generate(self, prompt: str):
        """Generates text from a prompt.

        Args:
        prompt: The prompt to generate text from.

        Returns:
        The generated text.
        """
        return self.model.generate_content(prompt, stream=False)

    """Categorises tokens in a batch

    Args:
    df: contains tokens to be categorised.  It is expected the have 'name' and 'symbol columns

    Returns:
    Categorised tokens in a JSON formatted string
    """
    def categoriseTokens(self, df: pd.DataFrame):
        promt = ""
        for idx, tk in df.iterrows():
          promt = f"{promt} {tk['name']} token with symbol '{tk['symbol']}', "

        response = client.generate(promt)
        # The model responds in the format ```json{actual.jsonData}```
        # Remove ```json prefix & ``` sufix
        _json = response.text.strip()
        _json = _json.removeprefix("```json")
        _json = _json.removesuffix("```")
        return _json
### 2.2. Tools to interact with datasets in BigQuery
# @title 2.2.1. List of tokens that are not meme; CURATED MANUALLY

EXCLUSION_LIST=(
 "So11111111111111111111111111111111111111112",
 "JUPyiwrYJFskUPiHa7hkeR8VUtAeFoSYbKedZNsDvCN",
 "HZ1JovNiVvGrGNiiYvEozEVgZ58xaU3RKwX8eACQBCt3",
 "85VBFQZC9TZkfaptBWjvUw7YbZjy52A6mjtPGjstQAmQ",
 "jtojtomepa8beP8AuQc6eXt5FriJwfFMwQx2v2f9mCL",
 "EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v",
 "Es9vMFrzaCERmJfrF4H2FYD4KCoNkY11McCe8BenwNYB",
 "J1toso1uCk3RLmjorhTtrVwY9HJ7X8V9yYac6Y7kGCPn",
 "5fTwKZP2AK39LtFN9Ayppu6hdCVKfMGVm79F2EgHCtsi",
 "TNSRxcUxoT9xBG3de7PiJyTDYu7kskLqcpddxnEJAS6",
 "NeonTjSjsuo3rexg9o6vHuMXw62f9V7zvmu8M8Zut44",
 "4k3Dyjzvzp8eMZWUXbBCjEvwSkkk59S5iCNLY3QrkX6R"
)



# @title 2.2.2. Write DataFrame to a table

from os import replace
from google.cloud import bigquery
import pandas
import pandas_gbq

MEME_KOINS="meme_koins"

# Create a dataset
def createDataset(project_id=GCP_PROJECT, region=GCP_REGION, dataset_id=MEME_KOINS):
  client = bigquery.Client(project=project_id)
  dataset = bigquery.Dataset(project_id + "." + dataset_id)
  dataset.location = region
  dataset = client.create_dataset(dataset, exists_ok=True)
  print(f"Dataset {dataset.dataset_id} created.")

def writeTableToBQ(table_id: str, df, project_id=GCP_PROJECT, dataset_id=MEME_KOINS):
  createDataset(project_id=project_id,dataset_id=dataset_id)
  _table_id = f"{project_id}.{dataset_id}.{table_id}"
  pandas_gbq.to_gbq(df, destination_table=_table_id, if_exists="append")
# @title 2.2.3. Tokens created in the last 4 years

%%bigquery tokens_over_last_4_years
SELECT
  name,
  symbol,
  mint,
  FORMAT_TIMESTAMP('%d-%m-%Y', block_timestamp) AS minted_on
FROM
  `bigquery-public-data.crypto_solana_mainnet_us.Tokens` as Tokens
WHERE
  DATE(Tokens.block_timestamp) >= DATE_SUB(CURRENT_DATE(), INTERVAL 4 YEAR)
  AND is_nft = False
  AND mint not in (
 "So11111111111111111111111111111111111111112",
 "JUPyiwrYJFskUPiHa7hkeR8VUtAeFoSYbKedZNsDvCN",
 "HZ1JovNiVvGrGNiiYvEozEVgZ58xaU3RKwX8eACQBCt3",
 "85VBFQZC9TZkfaptBWjvUw7YbZjy52A6mjtPGjstQAmQ",
 "jtojtomepa8beP8AuQc6eXt5FriJwfFMwQx2v2f9mCL",
 "EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v",
 "Es9vMFrzaCERmJfrF4H2FYD4KCoNkY11McCe8BenwNYB",
 "J1toso1uCk3RLmjorhTtrVwY9HJ7X8V9yYac6Y7kGCPn",
 "5fTwKZP2AK39LtFN9Ayppu6hdCVKfMGVm79F2EgHCtsi",
 "TNSRxcUxoT9xBG3de7PiJyTDYu7kskLqcpddxnEJAS6",
 "NeonTjSjsuo3rexg9o6vHuMXw62f9V7zvmu8M8Zut44",
 "4k3Dyjzvzp8eMZWUXbBCjEvwSkkk59S5iCNLY3QrkX6R"
)
# @title 2.2.4. Token Transfers over the last 4 year period
%%bigquery token_transfers_over_4_years
SELECT
   FORMAT_TIMESTAMP('%d-%m-%Y', TokenTransfers.block_timestamp) AS transfer_date,
   mint,
   count(*) AS total_trades
 FROM
   `bigquery-public-data.crypto_solana_mainnet_us.Token Transfers` AS TokenTransfers
 WHERE
  DATE(TokenTransfers.block_timestamp) >= DATE_SUB(CURRENT_DATE(), INTERVAL 4 YEAR)
  AND TokenTransfers.transfer_type in ("spl-transfer")
  AND TokenTransfers.mint not in (
 "So11111111111111111111111111111111111111112",
 "JUPyiwrYJFskUPiHa7hkeR8VUtAeFoSYbKedZNsDvCN",
 "HZ1JovNiVvGrGNiiYvEozEVgZ58xaU3RKwX8eACQBCt3",
 "85VBFQZC9TZkfaptBWjvUw7YbZjy52A6mjtPGjstQAmQ",
 "jtojtomepa8beP8AuQc6eXt5FriJwfFMwQx2v2f9mCL",
 "EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v",
 "Es9vMFrzaCERmJfrF4H2FYD4KCoNkY11McCe8BenwNYB",
 "J1toso1uCk3RLmjorhTtrVwY9HJ7X8V9yYac6Y7kGCPn",
 "5fTwKZP2AK39LtFN9Ayppu6hdCVKfMGVm79F2EgHCtsi",
 "TNSRxcUxoT9xBG3de7PiJyTDYu7kskLqcpddxnEJAS6",
 "NeonTjSjsuo3rexg9o6vHuMXw62f9V7zvmu8M8Zut44",
 "4k3Dyjzvzp8eMZWUXbBCjEvwSkkk59S5iCNLY3QrkX6R"
  )
 GROUP BY 1, 2
# @markdown ### 2.3. Pricing Utility

import requests
# API_KEY for birdeye.so
API_KEY="42f063bc46404ebeaab5a32cce7d6943"
def get_historical_price(token_mint_address:str, time_from: str, time_to: str, api_key=API_KEY, chain="solana"):
  url = f"https://public-api.birdeye.so/defi/history_price?address={token_mint_address}&address_type=token&type=1D&time_from={time_from}&time_to={time_to}"
  headers = {
         "x-chain": "solana",
        "X-API-KEY": f"{api_key}"
      }
  response = requests.get(url, headers=headers)
  print(response.text)

## 3. Analytics Workflow
# @title 3.1. Categorise each token
# We are going to use Gen AI model to help us categorise tokens

# from datetime import datetime
# import time

# client = AIPlatform(project_id=GCP_PROJECT, region=GCP_REGION, model_id="gemini-1.5-pro-preview-0409")

# total_tokens = len(tokens_over_last_4_years)
# print(f"Starting categorisatoion of {total_tokens} tokens at: ", datetime.now().strftime("%H:%M:%S"))
# chunk_size=25000
# # tokens_index=0
# tokens_index=250010
# token_dict = []
# while (tokens_index+chunk_size) < total_tokens:
#   upper_limit=tokens_index+chunk_size
#   print(f"Categorising tokens from {tokens_index} to {upper_limit} at: ", datetime.now().strftime("%H:%M:%S"))
#   token_dict.append(client.categoriseTokens(tokens_over_last_4_years[tokens_index:upper_limit]))
#   tokens_index = upper_limit + 1

# if tokens_index<total_tokens:
#   print(f"Categorising tokens from {tokens_index} to {total_tokens} at: ", datetime.now().strftime("%H:%M:%S"))
#   token_dict.append(client.categoriseTokens(tokens_over_last_4_years[tokens_index:total_tokens]))


# @title 3.1.2. Update the Tokens to include category information
# import json
# import pandas as pd

# # tokens = tokens_over_last_4_years.reset_index()
# counter=0
# categorised_tokens_agg = pd.DataFrame(columns=["category", "symbol"])
# for token_dict_each in token_dict:
#   _json = token_dict_each
#   # Load JSON data into a DataFrame
#   print(counter)
#   _categorised_tokens = json.loads(_json)
#   categorised_tokens = pd.DataFrame.from_dict(_categorised_tokens, orient="index", columns=["category"])
#   categorised_tokens["symbol"]=categorised_tokens.index
#   categorised_tokens = categorised_tokens.reset_index(drop=True)
#   categorised_tokens_agg = pd.concat([categorised_tokens_agg, categorised_tokens])
#   # print(categorised_tokens.head())
#   counter = counter + 1

# # Merge categorised dataframe with the original token information
# # categorised_tokens_agg.head()
# tokens = pd.merge(tokens_over_last_4_years, categorised_tokens_agg, on="symbol")
# tokens.head()
# @title 3.2. Combine Token & Transfer table for easier data analysis
# import pandas as pd
# result = pd.merge(token_transfers_over_4_years, tokens, on="mint")

# result.head()
# @title 3.3. Write the table to BigQuery
# writeTableToBQ("token_transfers", result)
